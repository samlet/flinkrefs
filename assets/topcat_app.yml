version: "1"

tables:
    # data-format:
    #   {"user_id": "952483", "item_id":"310884", "category_id": "4580532",
    #       "behavior": "pv", "ts": "2017-11-27T00:00:00Z"}
    # 按照数据的格式声明了 5 个字段，除此之外，我们还通过计算列语法和 PROCTIME() 内置函数声明了一个产生处理时间的虚拟列。
    # 我们还通过 WATERMARK 语法，在 ts 字段上声明了 watermark 策略（容忍5秒乱序），
    # ts 字段因此也成了事件时间列。
    user_behavior_kf:
        create: |
            CREATE TABLE user_behavior (
                user_id BIGINT,
                item_id BIGINT,
                category_id BIGINT,
                behavior STRING,
                ts TIMESTAMP(3),
                proctime AS PROCTIME(),   -- generates processing-time attribute using computed column
                WATERMARK FOR ts AS ts - INTERVAL '5' SECOND  -- defines watermark on ts column, marks ts as event-time attribute
            ) WITH (
                'connector' = 'kafka',  -- using kafka connector
                'topic' = 'user_behavior',  -- kafka topic
                'scan.startup.mode' = 'earliest-offset',  -- reading from the beginning
                'properties.bootstrap.servers' = 'localhost:9092',  -- kafka broker address
                'format' = 'json'  -- the data format is json
            )

    # 创建一个 ES 结果表，根据场景需求主要需要保存两个数据：小时、成交量
    # 不需要在 Elasticsearch 中事先创建 buy_cnt_per_hour 索引，Flink Job 会自动创建该索引。
    buy_cnt_per_hour_es:
        create: |
            CREATE TABLE buy_cnt_per_hour (
                hour_of_day BIGINT,
                buy_cnt BIGINT
            ) WITH (
                'connector' = 'elasticsearch-7', -- using elasticsearch connector
                'hosts' = 'http://localhost:9200',  -- elasticsearch address
                'index' = 'buy_cnt_per_hour'  -- elasticsearch index name, similar to database table name
            )

    # 统计一天每10分钟累计独立用户数:
    # 另一个有意思的可视化是统计一天中每一刻的累计独立用户数（uv），也就是每一刻的 uv 数都代表
    # 从0点到当前时刻为止的总计 uv 数，因此该曲线肯定是单调递增的。
    # 我们仍然先在 SQL CLI 中创建一个 Elasticsearch 表，用于存储结果汇总数据。主要字段有：
    # 日期时间和累积 uv 数。我们将日期时间作为 Elasticsearch 中的 document id，便于更新该日期时间的 uv 值。
    cumulative_uv_es:
        create: |
            CREATE TABLE cumulative_uv (
                date_str STRING,
                time_str STRING,
                uv BIGINT,
                PRIMARY KEY (date_str, time_str) NOT ENFORCED
            ) WITH (
                'connector' = 'elasticsearch-7',
                'hosts' = 'http://localhost:9200',
                'index' = 'cumulative_uv'
            )

    # 顶级类目排行榜:
    # 最后一个有意思的可视化是类目排行榜，从而了解哪些类目是支柱类目。
    # 不过由于源数据中的类目分类太细（约5000个类目），对于排行榜意义不大，因此我们希望能将其归约到顶级类目。
    # 所以在 mysql 容器中预先准备了子类目与顶级类目的映射数据，用作维表。
    category_dim_sql:
        create: |
            CREATE TABLE category_dim (
                sub_category_id BIGINT,
                parent_category_name STRING
            ) WITH (
                'connector' = 'jdbc',
                'url' = 'jdbc:mysql://localhost:3306/flink_test',
                'table-name' = 'category',
                'username' = 'root',
                'password' = 'root',
                'lookup.cache.max-rows' = '5000',
                'lookup.cache.ttl' = '10min'
            )

    # 创建一个 Elasticsearch 表，用于存储类目统计结果
    top_category_es:
        create: |
            CREATE TABLE top_category (
                category_name STRING PRIMARY KEY NOT ENFORCED,
                buy_cnt BIGINT
            ) WITH (
                'connector' = 'elasticsearch-7',
                'hosts' = 'http://localhost:9200',
                'index' = 'top_category'
            );

    # 通过维表关联，补全类目名称。我们仍然使用 CREATE VIEW 将该查询注册成一个视图，简化逻辑。
    # 维表关联使用 temporal join 语法，可以查看文档了解更多：
    # https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/streaming/joins.html#join-with-a-temporal-table
    rich_user_behavior_v:
        create: |
            CREATE VIEW rich_user_behavior AS
            SELECT U.user_id, U.item_id, U.behavior, C.parent_category_name as category_name
                FROM user_behavior AS U LEFT JOIN category_dim FOR SYSTEM_TIME AS OF U.proctime AS C
                ON U.category_id = C.sub_category_id;


pipes:
    # 统计每小时的成交量就是每小时共有多少 "buy" 的用户行为。因此会需要用到 TUMBLE 窗口函数，
    # 按照一小时切窗。然后每个窗口分别统计 "buy" 的个数，这可以通过先过滤出 "buy" 的数据，
    # 然后 COUNT(*) 实现。
    # 这里我们使用 HOUR 内置函数，从一个 TIMESTAMP 列中提取出一天中第几个小时的值。
    # 使用了 INSERT INTO将 query 的结果持续不断地插入到上文定义的 es 结果表中
    # （可以将 es 结果表理解成 query 的物化视图）。另外可以阅读该文档了解更多
    # 关于窗口聚合的内容：https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/queries.html#group-windows
    # 可以看到凌晨是一天中成交量的低谷。
    buy_cnt_per_hour_i:
        sql: |
            INSERT INTO buy_cnt_per_hour
                SELECT HOUR(TUMBLE_START(ts, INTERVAL '1' HOUR)), COUNT(*)
                    FROM user_behavior
                    WHERE behavior = 'buy'
                    GROUP BY TUMBLE(ts, INTERVAL '1' HOUR);

    buy_cnt_per_minute_i:
        sql: |
            INSERT INTO buy_cnt_per_hour
                SELECT MINUTE(TUMBLE_START(ts, INTERVAL '1' MINUTE)), COUNT(*)
                    FROM user_behavior
                    WHERE behavior = 'buy'
                    GROUP BY TUMBLE(ts, INTERVAL '1' MINUTE);

    # 为了实现该曲线，我们先抽取出日期和时间字段，我们使用 DATE_FORMAT 抽取出基本的日期与时间，
    # 再用 SUBSTR 和 字符串连接函数 || 将时间修正到10分钟级别，如: 12:10, 12:20。
    # 其次，我们在外层查询上基于日期分组，求当前最大的时间，和 UV，写入到 Elasticsearch 的索引中。
    # UV 的统计我们通过内置的 COUNT(DISTINCT user_id)来完成，Flink SQL 内部对 COUNT DISTINCT
    # 做了非常多的优化，因此可以放心使用。
    # 这里之所以需要求最大的时间，同时又按日期+时间作为主键写入到 Elasticsearch，是因为我们在计算累积 UV 数。
    #
    # 提交上述查询后，在 Kibana 中创建 cumulative_uv 的 index pattern，
    # 然后在 Dashboard 中创建一个"Line"折线图，选择 cumulative_uv 索引，
    # 按照如下截图中的配置（左侧）画出累计独立用户数曲线，并保存。
    cumulative_uv_i:
        sql: |
            INSERT INTO cumulative_uv
            SELECT date_str, MAX(time_str), COUNT(DISTINCT user_id) as uv
            FROM (
              SELECT
                DATE_FORMAT(ts, 'yyyy-MM-dd') as date_str,
                SUBSTR(DATE_FORMAT(ts, 'HH:mm'),1,4) || '0' as time_str,
                user_id
              FROM user_behavior)
            GROUP BY date_str;

    # 根据类目名称分组，统计出 buy 的事件数，并写入 Elasticsearch 中
    # 提交上述查询后，在 Kibana 中创建 top_category 的 index pattern，
    # 然后在 Dashboard 中创建一个"Horizontal Bar"条形图，选择 top_category 索引，
    # 按照如下截图中的配置（左侧）画出类目排行榜，并保存。
    # 可以看到服饰鞋包的成交量远远领先其他类目。
    top_category_i:
        sql: |
            INSERT INTO top_category
                SELECT category_name, COUNT(*) buy_cnt
                    FROM rich_user_behavior
                    WHERE behavior = 'buy'
                    GROUP BY category_name;
