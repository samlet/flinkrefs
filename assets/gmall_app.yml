version: "1"

tables:
    product_stats:
        create: |
            CREATE TABLE product_stats (spu_name STRING,
                click_ct BIGINT,
                cart_ct BIGINT,
                order_ct BIGINT,
                stt STRING,edt STRING )
            WITH ( 'connector' = 'kafka',
                'topic' = 'dws_product_stats',
                'properties.bootstrap.servers' = 'localhost:9092',
                'properties.group.id' = 'keyword_stats_app',
                'format' = 'json',
                'scan.startup.mode' = 'latest-offset' )

    page_view:
        create: |
            CREATE TABLE page_view (
                common MAP<STRING, STRING>,
                page MAP<STRING, STRING>,
                ts BIGINT,
                rowtime as TO_TIMESTAMP(FROM_UNIXTIME(ts/1000,'yyyy-MM-dd HH:mm:ss')),
                WATERMARK FOR rowtime AS rowtime - INTERVAL '2' SECOND)
            WITH ( 'connector' = 'kafka',
                'topic' = 'dwd_page_log',
                'properties.bootstrap.servers' = 'localhost:9092',
                'properties.group.id' = 'keywordstats_app_group',
                'format' = 'json',
                'scan.startup.mode' = 'latest-offset' )

    order_wide:
        create: |
            CREATE TABLE ORDER_WIDE (province_id BIGINT,
                province_name STRING,province_area_code STRING,
                province_iso_code STRING,province_3166_2_code STRING,
                order_id STRING,
                split_total_amount DOUBLE,create_time STRING,rowtime AS TO_TIMESTAMP(create_time) ,
                WATERMARK FOR  rowtime  AS rowtime)
            WITH ( 'connector' = 'kafka',
                'topic' = 'dwm_order_wide',
                'properties.bootstrap.servers' = 'localhost:9092',
                'properties.group.id' = 'province_stats',
                'format' = 'json',
                'scan.startup.mode' = 'latest-offset' )

pipes:
    # 从商品统计中获取关键词
    # sink to clickhouse:
    # insert into keyword_stats_0820(keyword,ct,source,stt,edt,ts) values(?,?,?,?,?,?)
    keyword_stats_product:
        udf: [ik_analyze, keywordProductC2R]
        sql: |
            select keyword,ct,source,
                DATE_FORMAT(stt,'yyyy-MM-dd HH:mm:ss')  stt,
                DATE_FORMAT(edt,'yyyy-MM-dd HH:mm:ss') as edt,
                UNIX_TIMESTAMP()*1000 ts
            from product_stats ,
                LATERAL TABLE(ik_analyze(spu_name)) as T(keyword) ,
                LATERAL TABLE(keywordProductC2R( click_ct ,cart_ct,order_ct)) as T2(ct,source)

    # 从动态表中查询数据  --->尚硅谷大数据数仓-> [尚, 硅谷, 大, 数据, 数, 仓]
    fullword:
        sql: |
            select page['item'] fullword,rowtime
            from page_view
            where page['page_id']='good_list' and page['item'] IS NOT NULL

    # 利用自定义函数  对搜索关键词进行拆分
    keyword_table:
        sql: |
            SELECT keyword, rowtime
            FROM  fullword, LATERAL TABLE(ik_analyze(fullword)) AS t(keyword)

    # insert into keyword_stats_0820(keyword,ct,source,stt,edt,ts) values(?,?,?,?,?,?)
    reduce_table:
        sql: |
            select keyword,count(*) ct,  'SEARCH' source,
                DATE_FORMAT(TUMBLE_START(rowtime, INTERVAL '10' SECOND),'yyyy-MM-dd HH:mm:ss') stt,
                DATE_FORMAT(TUMBLE_END(rowtime, INTERVAL '10' SECOND),'yyyy-MM-dd HH:mm:ss') edt ,
                UNIX_TIMESTAMP()*1000 ts
            from keyword_table
            group by TUMBLE(rowtime, INTERVAL '10' SECOND), keyword

    # 使用FlinkSQL对地区主题统计
    # insert into  province_stats_0820  values(?,?,?,?,?,?,?,?,?,?)
    province_state_table:
        sql: |
            select
                DATE_FORMAT(TUMBLE_START(rowtime, INTERVAL '10' SECOND ),'yyyy-MM-dd HH:mm:ss') stt,
                DATE_FORMAT(TUMBLE_END(rowtime, INTERVAL '10' SECOND ),'yyyy-MM-dd HH:mm:ss') edt ,
                province_id,province_name,province_area_code area_code,
                province_iso_code iso_code ,province_3166_2_code iso_3166_2 ,
                COUNT( DISTINCT  order_id) order_count, sum(split_total_amount) order_amount,
                UNIX_TIMESTAMP()*1000 ts
            from  ORDER_WIDE
            group by
                TUMBLE(rowtime, INTERVAL '10' SECOND ),
                province_id,province_name,province_area_code,province_iso_code,province_3166_2_code

